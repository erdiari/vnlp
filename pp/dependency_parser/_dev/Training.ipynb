{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "     |████████████████████████████████| 24.1 MB 4.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from gensim) (0.8)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "     |████████████████████████████████| 58 kB 13.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from gensim) (1.5.3)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from utils import (create_dependency_parser_model, process_single_sentence, data_generator, LAS, UAS, fit_label_tokenizer, load_triplets)\n",
    "\n",
    "CHECKPOINT_DIR = \"saved_models/weights.{epoch:02d}-{loss:.4f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"UD/refined-UD-Turkish/**//*.conllu\", recursive = True)\n",
    "\n",
    "train_files = [file for file in files if \"train\" in file]\n",
    "dev_files = [file for file in files if \"dev\" in file]\n",
    "test_files = [file for file in files if \"test\" in file]\n",
    "\"\"\"\n",
    "train_files = [file for file in files if \"tr_imst-ud-test\" not in file]\n",
    "test_files = [file for file in files if \"tr_imst-ud-test\" in file]\n",
    "\"\"\"\n",
    "\n",
    "SENTENCE_MAX_LEN = 40\n",
    "TAG_MAX_LEN = 15\n",
    "\n",
    "word_oov_token = '<OOV>'\n",
    "word_embedding_model = 'Word2Vec_medium.model'\n",
    "\n",
    "\"\"\"\n",
    "# Fit tokenizer_label once only\n",
    "tokenizer_label = fit_label_tokenizer(files)\n",
    "\n",
    "# saving tokenizer_label\n",
    "with open('tokenizer_label.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\"\n",
    "\n",
    "with open('tokenizer_label.pickle', 'rb') as handle:\n",
    "    tokenizer_label = pickle.load(handle)\n",
    "    \n",
    "with open('tokenizer_tag.pickle', 'rb') as handle: # This is transferred from StemmerAnalyzer\n",
    "    tokenizer_tag = pickle.load(handle)\n",
    "    \n",
    "sentence_max_len = SENTENCE_MAX_LEN\n",
    "tag_max_len = TAG_MAX_LEN\n",
    "arc_label_vector_len = sentence_max_len + len(tokenizer_label.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT saving tokenizer_word until it is finalized\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "ft_model = Word2Vec.load(word_embedding_model)\n",
    "word_embedding_vocab_size = len(ft_model.wv.key_to_index)\n",
    "word_embedding_vector_size = ft_model.vector_size\n",
    "\n",
    "embedding_vectors_ = ft_model.wv.vectors\n",
    "embedding_dict_ = ft_model.wv.key_to_index\n",
    "\n",
    "embedding_dict = {word_oov_token: 0}\n",
    "word_embedding_matrix = np.zeros((word_embedding_vocab_size + 1, word_embedding_vector_size))\n",
    "for k,v in embedding_dict_.items():\n",
    "    embedding_dict[k] = v + 1\n",
    "    word_embedding_matrix[v] = embedding_vectors_[v-1]\n",
    "    \n",
    "tokenizer_word = tf.keras.preprocessing.text.Tokenizer(filters = None, lower = False, oov_token = word_oov_token)\n",
    "tokenizer_word.word_index = embedding_dict\n",
    "tokenizer_word.index_word = {value:key for key, value in embedding_dict.items()}\n",
    "\n",
    "\"\"\"\n",
    "# saving tokenizer_label\n",
    "with open('tokenizer_word.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_word, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\"\n",
    "print('NOT saving tokenizer_word until it is finalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# saving exhausts diskspace\\nX_test_y_test = (X_test, y_test)\\nwith open('X_test_y_test.pickle', 'wb') as f:\\n    pickle.dump(X_test_y_test, f)\\n\\nwith open('X_test_y_test.pickle', 'rb') as handle:\\n    (X_test, y_test) = pickle.load(handle)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test_data = load_triplets(test_files)\n",
    "batch_of_words = []\n",
    "batch_of_tags = []\n",
    "batch_of_arc_label_vectors = []\n",
    "\n",
    "batch_of_left_context_words = []\n",
    "batch_of_left_context_tags = []\n",
    "batch_of_left_context_arc_label_vectors = []\n",
    "\n",
    "batch_of_right_context_words = []\n",
    "batch_of_right_context_tags = []\n",
    "for sentence in test_data:\n",
    "    # I won't use sentences longer than sentence_max_len for training\n",
    "    # because first token can depend on last token and truncating and breaking\n",
    "    # the tree structure can hurt training.\n",
    "    # Also there's no way to output arc greater than sentence_max_len\n",
    "    # so I will skip them\n",
    "    if len(sentence) > sentence_max_len:\n",
    "        continue\n",
    "    else:\n",
    "        (batch_of_words_, batch_of_tags_, batch_of_arc_label_vectors_, batch_of_left_context_words_,\n",
    "        batch_of_left_context_tags_, batch_of_left_context_arc_label_vectors_, batch_of_right_context_words_,\n",
    "        batch_of_right_context_tags_) = process_single_sentence(sentence, sentence_max_len, \n",
    "                                                                tag_max_len, arc_label_vector_len, \n",
    "                                                                tokenizer_word, tokenizer_tag, \n",
    "                                                                tokenizer_label)\n",
    "        batch_of_words += batch_of_words_\n",
    "        batch_of_tags += batch_of_tags_\n",
    "        batch_of_arc_label_vectors += batch_of_arc_label_vectors_\n",
    "\n",
    "        batch_of_left_context_words += batch_of_left_context_words_\n",
    "        batch_of_left_context_tags += batch_of_left_context_tags_\n",
    "        batch_of_left_context_arc_label_vectors += batch_of_left_context_arc_label_vectors_\n",
    "\n",
    "        batch_of_right_context_words += batch_of_right_context_words_\n",
    "        batch_of_right_context_tags += batch_of_right_context_tags_\n",
    "\n",
    "batch_of_words = np.array(batch_of_words)\n",
    "batch_of_tags = np.array(batch_of_tags)\n",
    "batch_of_arc_label_vectors = np.array(batch_of_arc_label_vectors)\n",
    "\n",
    "batch_of_left_context_words = np.array(batch_of_left_context_words)\n",
    "batch_of_left_context_tags = np.array(batch_of_left_context_tags)\n",
    "batch_of_left_context_arc_label_vectors = np.array(batch_of_left_context_arc_label_vectors)\n",
    "\n",
    "batch_of_right_context_words = np.array(batch_of_right_context_words)\n",
    "batch_of_right_context_tags = np.array(batch_of_right_context_tags)\n",
    "\n",
    "X_test = (batch_of_words, batch_of_tags, batch_of_left_context_words, batch_of_left_context_tags,\n",
    "        batch_of_left_context_arc_label_vectors, batch_of_right_context_words, batch_of_right_context_tags)\n",
    "y_test = batch_of_arc_label_vectors\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# saving exhausts diskspace\n",
    "X_test_y_test = (X_test, y_test)\n",
    "with open('X_test_y_test.pickle', 'wb') as f:\n",
    "    pickle.dump(X_test_y_test, f)\n",
    "\n",
    "with open('X_test_y_test.pickle', 'rb') as handle:\n",
    "    (X_test, y_test) = pickle.load(handle)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rnn_stacks = 2\n",
    "rnn_units_multiplier = 2\n",
    "tag_num_rnn_units = word_embedding_vector_size\n",
    "lc_num_rnn_units = tag_num_rnn_units * rnn_units_multiplier\n",
    "lc_arc_label_num_rnn_units = tag_num_rnn_units * rnn_units_multiplier\n",
    "rc_num_rnn_units = tag_num_rnn_units * rnn_units_multiplier\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_dependency_parser_model(word_embedding_vocab_size, word_embedding_vector_size, word_embedding_matrix,\n",
    "                                   sentence_max_len, tag_max_len, arc_label_vector_len, num_rnn_stacks, \n",
    "                                   tag_num_rnn_units, lc_num_rnn_units, lc_arc_label_num_rnn_units, rc_num_rnn_units,\n",
    "                                   dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      " 969/6119 [===>..........................] - ETA: 1:13:40 - loss: 0.0559 - LAS: 0.3367 - UAS: 0.4371"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 16 # determined by monitoring val_loss during development phase\n",
    "verbose = 1\n",
    "patience = 3\n",
    "\n",
    "base_lr = 0.001\n",
    "decay_rate = 0.95\n",
    "def scheduler(epoch, _):\n",
    "    if epoch < 2:\n",
    "        return base_lr\n",
    "    else:\n",
    "        lr = base_lr * (decay_rate ** epoch)\n",
    "        return lr\n",
    "    \n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = patience, verbose = verbose)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = CHECKPOINT_DIR, \n",
    "                                                save_freq = 'epoch', save_weights_only = True, verbose = 1)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [LAS, UAS], run_eagerly = True)\n",
    "\n",
    "# Continuing from checkpoint\n",
    "weight_files = os.listdir('saved_models')\n",
    "weight_files = [file for file in weight_files if \".hdf5\" in file]\n",
    "if len(weight_files) > 0:\n",
    "    last_epoch_was = pd.Series(weight_files).str.split('.').str[1].str.split('-').str[0].astype(int).max()\n",
    "    idx_of_weight_to_load = pd.Series(weight_files).str.split('.').str[1].str.split('-').str[0].astype(int).idxmax()\n",
    "\n",
    "    weight_to_load = weight_files[idx_of_weight_to_load]\n",
    "    print('Loading checkpoint of last epoch:', weight_to_load)\n",
    "    model.load_weights('saved_models/' + weight_to_load)\n",
    "else:\n",
    "    last_epoch_was = 0\n",
    "\n",
    "model.fit(data_generator(train_files + dev_files + test_files, tokenizer_word, tokenizer_tag, tokenizer_label, sentence_max_len, tag_max_len, arc_label_vector_len, batch_size), \n",
    "          batch_size = batch_size, epochs = epochs, verbose = verbose, steps_per_epoch = ((609_125 + 67_377 + 106_825) // batch_size),\n",
    "          initial_epoch = last_epoch_was, callbacks = [checkpoint, lr_scheduler]) #validation_data = (X_test, y_test), \n",
    "\n",
    "# Number of Words in:\n",
    "# train_files: 609_125\n",
    "# dev_files: 67_377\n",
    "# test_files: 106_825"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
