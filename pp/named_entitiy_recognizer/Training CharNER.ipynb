{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "DATA_PATH = \"../data/\"\n",
    "CHECKPOINT_DIR = \"saved_models/weights.{epoch:02d}-{val_loss:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_PATH + \"df_train_big.csv\", index_col = 0).drop('file', axis = 1)\n",
    "dev = pd.read_csv(DATA_PATH + \"df_dev.csv\", index_col = 0).drop('file', axis = 1)\n",
    "test = pd.read_csv(DATA_PATH + \"df_test.csv\", index_col = 0).drop('file', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Uzlaştırıcı politikasını cumhurbaşkanlığı döneminde de sürdürmesine karşın savaş sonunda Fransa ' da gerilimlerin önü alınamadı .\n",
      "Label: O O O O O O O O O LOC O O O O O O\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, train.shape[0], 1)[0]\n",
    "print('Text:', train.loc[idx, 'text'])\n",
    "print('Label:', train.loc[idx, 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = train.loc[:, 'text']\n",
    "y_train_ = train.loc[:, 'label']\n",
    "\n",
    "X_dev_ = dev.loc[:, 'text']\n",
    "y_dev_ = dev.loc[:, 'label']\n",
    "\n",
    "X_test_ = test.loc[:, 'text']\n",
    "y_test_ = test.loc[:, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char level processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_level_preprocess_string(string_X, string_y):\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    tokens = string_X.split()\n",
    "    labels = string_y.split()\n",
    "    for token, label in zip(tokens, labels):\n",
    "        chars_of_token = list(token) + [' ']\n",
    "        labels_of_chars_of_token = len(chars_of_token) * [label]\n",
    "\n",
    "        X_ += chars_of_token\n",
    "        y_ += labels_of_chars_of_token\n",
    "\n",
    "    # Remove last whitespace, it is product of code\n",
    "    del X_[-1]\n",
    "    del y_[-1]\n",
    "    \n",
    "    return X_, y_\n",
    "\n",
    "def char_level_preprocess_df(df_X, df_y):\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx in range(df_X.shape[0]):\n",
    "        X_ = []\n",
    "        y_ = []\n",
    "        tokens = df_X.loc[idx]\n",
    "        labels = df_y.loc[idx]\n",
    "        \n",
    "        X_, y_ = char_level_preprocess_string(tokens, labels)\n",
    "\n",
    "        X.append(X_)\n",
    "        y.append(y_)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = char_level_preprocess_df(X_train_, y_train_)\n",
    "X_dev, y_dev = char_level_preprocess_df(X_dev_, y_dev_)\n",
    "X_test, y_test = char_level_preprocess_df(X_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 140\n",
    "seq_len = 256\n",
    "oov_token = '<OOV>'\n",
    "padding_strat = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_X = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token = oov_token, \n",
    "                                                    filters = '', lower = False, char_level= True)\n",
    "tokenizer_y = tf.keras.preprocessing.text.Tokenizer(filters = '', lower = False)\n",
    "\n",
    "tokenizer_X.fit_on_texts(X_train)\n",
    "tokenizer_y.fit_on_texts(y_train)\n",
    "\n",
    "X_train = tokenizer_X.texts_to_sequences(X_train)\n",
    "X_dev = tokenizer_X.texts_to_sequences(X_dev)\n",
    "X_test = tokenizer_X.texts_to_sequences(X_test)\n",
    "\n",
    "y_train = tokenizer_y.texts_to_sequences(y_train)\n",
    "y_dev = tokenizer_y.texts_to_sequences(y_dev)\n",
    "y_test = tokenizer_y.texts_to_sequences(y_test)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen = seq_len, padding = padding_strat)\n",
    "X_dev = tf.keras.preprocessing.sequence.pad_sequences(X_dev, maxlen = seq_len, padding = padding_strat)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen = seq_len, padding = padding_strat)\n",
    "\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train, maxlen = seq_len, padding = padding_strat)\n",
    "y_dev = tf.keras.preprocessing.sequence.pad_sequences(y_dev, maxlen = seq_len, padding = padding_strat)\n",
    "y_test = tf.keras.preprocessing.sequence.pad_sequences(y_test, maxlen = seq_len, padding = padding_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 32\n",
    "rnn_dim = 128\n",
    "num_rnn_stacks = 5\n",
    "mlp_dim = 32\n",
    "num_classes = len(tokenizer_y.index_word) + 1\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = embed_size, input_length=seq_len)) #mask_zero=True, input_dim = vocab_size + 1\n",
    "\n",
    "for _ in range(num_rnn_stacks):\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_dim, return_sequences = True)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(mlp_dim, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(dropout))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CustomNonPaddingTokenLoss()\n",
    "model.compile(optimizer='adam', loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2552/2552 [==============================] - 820s 318ms/step - loss: 0.3127 - val_loss: 0.3050\n",
      "\n",
      "Epoch 00001: saving model to saved_models\\weights.01-0.31.hdf5\n",
      "Epoch 2/100\n",
      "2552/2552 [==============================] - 806s 316ms/step - loss: 0.1866 - val_loss: 0.2490\n",
      "\n",
      "Epoch 00002: saving model to saved_models\\weights.02-0.25.hdf5\n",
      "Epoch 3/100\n",
      "2552/2552 [==============================] - 806s 316ms/step - loss: 0.1633 - val_loss: 0.2264\n",
      "\n",
      "Epoch 00003: saving model to saved_models\\weights.03-0.23.hdf5\n",
      "Epoch 4/100\n",
      "2552/2552 [==============================] - 806s 316ms/step - loss: 0.1531 - val_loss: 0.2163\n",
      "\n",
      "Epoch 00004: saving model to saved_models\\weights.04-0.22.hdf5\n",
      "Epoch 5/100\n",
      "2552/2552 [==============================] - 806s 316ms/step - loss: 0.1483 - val_loss: 0.2185\n",
      "\n",
      "Epoch 00005: saving model to saved_models\\weights.05-0.22.hdf5\n",
      "Epoch 6/100\n",
      "2552/2552 [==============================] - 807s 316ms/step - loss: 0.1444 - val_loss: 0.2052\n",
      "\n",
      "Epoch 00006: saving model to saved_models\\weights.06-0.21.hdf5\n",
      "Epoch 7/100\n",
      "2552/2552 [==============================] - 808s 317ms/step - loss: 0.1416 - val_loss: 0.2024\n",
      "\n",
      "Epoch 00007: saving model to saved_models\\weights.07-0.20.hdf5\n",
      "Epoch 8/100\n",
      "2552/2552 [==============================] - 810s 317ms/step - loss: 0.1402 - val_loss: 0.2007\n",
      "\n",
      "Epoch 00008: saving model to saved_models\\weights.08-0.20.hdf5\n",
      "Epoch 9/100\n",
      "2552/2552 [==============================] - 862s 338ms/step - loss: 0.1390 - val_loss: 0.1956\n",
      "\n",
      "Epoch 00009: saving model to saved_models\\weights.09-0.20.hdf5\n",
      "Epoch 10/100\n",
      "2552/2552 [==============================] - 826s 324ms/step - loss: 0.1380 - val_loss: 0.1952\n",
      "\n",
      "Epoch 00010: saving model to saved_models\\weights.10-0.20.hdf5\n",
      "Epoch 11/100\n",
      "2552/2552 [==============================] - 808s 316ms/step - loss: 0.1373 - val_loss: 0.1943\n",
      "\n",
      "Epoch 00011: saving model to saved_models\\weights.11-0.19.hdf5\n",
      "Epoch 12/100\n",
      "2552/2552 [==============================] - 809s 317ms/step - loss: 0.1374 - val_loss: 0.2008\n",
      "\n",
      "Epoch 00012: saving model to saved_models\\weights.12-0.20.hdf5\n",
      "Epoch 13/100\n",
      "2235/2552 [=========================>....] - ETA: 1:38 - loss: 0.1365"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "verbose = 1\n",
    "patience = 5\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = patience, verbose = verbose, restore_best_weights= True)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = CHECKPOINT_DIR, save_freq = 'epoch', save_weights_only = True, verbose = 1)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size, epochs, verbose, callbacks = [model_checkpoint_callback, early_stopping], validation_data = (X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout 0.3: min_val_loss: 0.1579\n",
    "\n",
    "Dropout 0.2: min_val_loss: 0.1581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_weights = pd.Series(os.listdir('saved_models'))\n",
    "weight_to_load = saved_weights[saved_weights.str.contains('hdf5')].values[0]\n",
    "model.load_weights('saved_models/' + weight_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will turn into Inference Pipeline\n",
    "def predict_char_level(word_punct_tokenized):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    word_punct_tokenized List(str): List of tokens (WordPunct level)\n",
    "    i.e: [\"İstanbul\", \"'\", \"da\", \"yaşıyorum\", \".\"]\n",
    "    \"\"\"\n",
    "    white_space_joined_word_punct_tokens = \" \".join(word_punct_tokenized)\n",
    "    sequences = tokenizer_X.texts_to_sequences([white_space_joined_word_punct_tokens])\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen = seq_len, padding = padding_strat)\n",
    "    raw_pred = model.predict([padded])\n",
    "    arg_max_pred = tf.math.argmax(raw_pred, axis = 2).numpy().reshape(-1)\n",
    "    \n",
    "    return arg_max_pred #tokenizer_y.sequences_to_texts([arg_max_pred])\n",
    "\n",
    "def charner_decoder(word_punct_tokenized, arg_max_pred):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    word_punct_tokenized: List(str) : List of tokens (WordPunct level)\n",
    "    i.e: [\"İstanbul\", \"'\", \"da\", \"yaşıyorum\", \".\"]\n",
    "    \n",
    "    arg_max_pred: List(int) : argmax(axis = -1) of model output\n",
    "    \n",
    "    Output:\n",
    "    decoded_entities: List(str) : List of entities, one entity per token\n",
    "    \"\"\"\n",
    "    \n",
    "    lens = [0] + [len(token) + 1 for token in word_punct_tokenized]\n",
    "    cumsum_of_lens = np.cumsum(lens)\n",
    "    \n",
    "    decoded_entities = []\n",
    "    for idx in range(len(cumsum_of_lens) - 1):\n",
    "        lower_bound = cumsum_of_lens[idx]\n",
    "        upper_bound = cumsum_of_lens[idx + 1]\n",
    "\n",
    "        island = arg_max_pred[lower_bound:upper_bound]\n",
    "        mode_value = stats.mode(island).mode[0]\n",
    "        detokenized_pred = tokenizer_y.sequences_to_texts([[mode_value]])[0]\n",
    "        decoded_entities.append(detokenized_pred)\n",
    "        \n",
    "    return decoded_entities\n",
    "\n",
    "def pipeline(text):\n",
    "    word_punct_tokenized = WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "    # if len chars (including whitespaces) > sequence length, split it recursively\n",
    "    len_text = len(list(\" \".join(word_punct_tokenized)))\n",
    "    if len_text > seq_len:\n",
    "        \n",
    "        num_tokens = len(word_punct_tokenized)\n",
    "        first_half_tokens, first_half_entities = pipeline(\" \".join(word_punct_tokenized[:num_tokens // 2]))\n",
    "        second_half_tokens, second_half_entities = pipeline(\" \".join(word_punct_tokenized[(num_tokens // 2):]))\n",
    "\n",
    "        word_punct_tokenized = first_half_tokens + second_half_tokens\n",
    "        decoded_entities = first_half_entities + second_half_entities\n",
    "\n",
    "    else:\n",
    "        charlevel_pred = predict_char_level(word_punct_tokenized)\n",
    "        decoded_entities = charner_decoder(word_punct_tokenized, charlevel_pred)\n",
    "        \n",
    "    return word_punct_tokenized, decoded_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test = []\n",
    "y_hat = []\n",
    "for idx in test.index.values:\n",
    "    text = test.loc[idx, 'text']\n",
    "    #len_text = len(list(text))\n",
    "    #if len_text <= seq_len:\n",
    "    label = test.loc[idx, 'label'].split()\n",
    "\n",
    "    _, pred = pipeline(text)\n",
    "\n",
    "    y_test += label\n",
    "    y_hat += pred\n",
    "    \n",
    "assert len(y_test) == len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, y_hat):\n",
    "    accuracy = metrics.accuracy_score(y_test, y_hat)\n",
    "    f1_macro = metrics.f1_score(y_test, y_hat, average = 'macro')\n",
    "    f1_micro = metrics.f1_score(y_test, y_hat, average = 'micro')\n",
    "    \n",
    "    df_results = pd.DataFrame([accuracy, f1_macro, f1_micro], index = ['accuracy', 'f1_macro', 'f1_micro'], columns = ['value']).T\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 dropout result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <td>0.963643</td>\n",
       "      <td>0.911957</td>\n",
       "      <td>0.963643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  f1_macro  f1_micro\n",
       "value  0.963643  0.911957  0.963643"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 dropout result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <td>0.962474</td>\n",
       "      <td>0.908618</td>\n",
       "      <td>0.962474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  f1_macro  f1_micro\n",
       "value  0.962474  0.908618  0.962474"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SavasYildirim BERTNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wikiann_test = pd.read_csv(DATA_PATH + \"df_test.csv\", index_col = 0)\n",
    "df_wikiann_test = df_wikiann_test.loc[df_wikiann_test['file'] == 'wikiann-test.txt'].reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test = []\n",
    "y_hat = []\n",
    "for idx in df_wikiann_test.index.values:\n",
    "    text = df_wikiann_test.loc[idx, 'text']\n",
    "    #len_text = len(list(text))\n",
    "    #if len_text <= seq_len:\n",
    "    label = df_wikiann_test.loc[idx, 'label'].split()\n",
    "\n",
    "    _, pred = pipeline(text)\n",
    "\n",
    "    y_test += label\n",
    "    y_hat += pred\n",
    "    \n",
    "assert len(y_test) == len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <td>0.949852</td>\n",
       "      <td>0.90326</td>\n",
       "      <td>0.949852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  f1_macro  f1_micro\n",
       "value  0.949852   0.90326  0.949852"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "- Dropout rate arttırarak daha düşük bir val_loss elde etmeye çalış.\n",
    "- TWNERTC ile ve olmadan da train edip val_loss'a bak.\n",
    "\n",
    "\n",
    "**DONE:**\n",
    "\n",
    "- BOYUTU 256'dan büyük olanları ortadan 2'ye bölüp 2 kez yap. Bunu recursive br şekilde yapıp birleştirebilirim\n",
    "- Literatürden modellerle kıyaslamaya çalış\n",
    "- Biraz daha büyük bir model train etsem sonuç nasıl değişir?\n",
    "- CustomLoss ile değil de default catloss ile eğitsem sonuç nasıl değişir?\n",
    "- mask_zero ile eğitsem sonuç nasıl değişir?\n",
    "- Word-Level train etsem sonuç nasıl değişir?\n",
    "- RNN size 128 units yapıp 5 layer ile train et\n",
    "- savasyildirim'in metrik'leri ile kıyasla\n",
    "- Aynı architecture ile WORDNer train edip metric'lere bak: CharNER is slightly better than WORDNer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Meryem', 'PER'),\n",
       " ('Beşer', 'PER'),\n",
       " ('ile', 'O'),\n",
       " ('birlikte', 'O'),\n",
       " ('önce', 'O'),\n",
       " ('Bursa', 'LOC'),\n",
       " (\"'\", 'O'),\n",
       " ('ya', 'O'),\n",
       " ('oradan', 'O'),\n",
       " ('da', 'O'),\n",
       " ('İzmir', 'LOC'),\n",
       " (\"'\", 'O'),\n",
       " ('e', 'O'),\n",
       " ('gittik', 'O'),\n",
       " (',', 'O'),\n",
       " ('haftasonunu', 'O'),\n",
       " ('Foça', 'LOC'),\n",
       " (\"'\", 'O'),\n",
       " ('da', 'O'),\n",
       " ('geçirdik', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Meryem Beşer ile birlikte önce Bursa'ya oradan da İzmir'e gittik, haftasonunu Foça'da geçirdik.\"\n",
    "tokens, entities = pipeline(text)\n",
    "[(t,e) for t,e in zip(tokens, entities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cumhurbaşkanı', 'O'),\n",
       " ('Ahmet', 'PER'),\n",
       " ('Necdet', 'PER'),\n",
       " ('Sezer', 'PER'),\n",
       " (\"'\", 'O'),\n",
       " ('in', 'O'),\n",
       " ('açıklamalarına', 'O'),\n",
       " ('göre', 'O'),\n",
       " ('Ankara', 'LOC'),\n",
       " (\"'\", 'O'),\n",
       " ('daki', 'O'),\n",
       " ('TBMM', 'ORG'),\n",
       " ('3', 'O'),\n",
       " ('gün', 'O'),\n",
       " ('daha', 'O'),\n",
       " ('tatil', 'O'),\n",
       " ('kalacak', 'O')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Cumhurbaşkanı Ahmet Necdet Sezer'in açıklamalarına göre Ankara'daki TBMM 3 gün daha tatil kalacak\"\n",
    "tokens, entities = pipeline(text)\n",
    "[(t,e) for t,e in zip(tokens, entities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ben', 'O'),\n",
       " ('Melikşah', 'PER'),\n",
       " (',', 'O'),\n",
       " ('28', 'O'),\n",
       " ('yaşındayım', 'O'),\n",
       " (',', 'O'),\n",
       " ('İstanbul', 'LOC'),\n",
       " (\"'\", 'O'),\n",
       " ('da', 'O'),\n",
       " ('ikamet', 'O'),\n",
       " ('ediyorum', 'O'),\n",
       " ('ve', 'O'),\n",
       " ('VNGRS', 'ORG'),\n",
       " ('AI', 'ORG'),\n",
       " ('Takımı', 'ORG'),\n",
       " (\"'\", 'O'),\n",
       " ('nda', 'O'),\n",
       " ('çalışıyorum', 'O')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Ben Melikşah, 28 yaşındayım, İstanbul'da ikamet ediyorum ve VNGRS AI Takımı'nda çalışıyorum\"\n",
    "tokens, entities = pipeline(text)\n",
    "[(t,e) for t,e in zip(tokens, entities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
