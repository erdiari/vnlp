{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "from melik_utils import (create_model, convert_data_to_sentence_form, \n",
    "                         fit_tokenizer_char, fit_tokenizer_tag, process_data,\n",
    "                         data_generator)\n",
    "from yildiz_data_utils import sentence_generator, capitalize\n",
    "from yildiz_analyzer import TurkishStemSuffixCandidateGenerator\n",
    "\n",
    "CHECKPOINT_DIR = \"saved_models/weights.{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "train_files = glob.glob(\"data/train/*.txt\", recursive = True)\n",
    "test_files = glob.glob(\"data/test/*.txt\", recursive = True)\n",
    "all_files = train_files + test_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max_analysis = 10 # 0.99 quantile\n",
    "\n",
    "# Stem\n",
    "stem_max_len = 10 # 0.99 quantile\n",
    "tokenizer_char_oov = '<OOV>'\n",
    "\n",
    "# Tag\n",
    "tag_max_len = 15 # 0.99 quantile\n",
    "tokenizer_tag_oov = '<OOV>'\n",
    "\n",
    "# Left and Right Surface Context\n",
    "sentence_max_len = 40 # 0.95 quantile is 42\n",
    "surface_token_max_len = 15 # 0.99 quantile\n",
    "\n",
    "# Data preparing related\n",
    "exclude_unambigious = False\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2090\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "#test = convert_data_to_sentence_form(test_files[0])\n",
    " \n",
    "test = []\n",
    "for file in test_files:\n",
    "    for sentence in sentence_generator(file):\n",
    "        test.append(sentence)\n",
    "\n",
    "        \n",
    "test_2006 = []\n",
    "for sentence in sentence_generator('data/trmorph2006_test.txt'):\n",
    "    test_2006.append(sentence)\n",
    "\n",
    "print(len(test))\n",
    "print(len(test_2006))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting tokenizers\n",
    "\"\"\"\n",
    "tokenizer_char = fit_tokenizer_char(all_files)\n",
    "tokenizer_tag = fit_tokenizer_tag(all_files)\n",
    "\n",
    "\n",
    "# saving tokenizers\n",
    "with open('tokenizer_char.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_char, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('tokenizer_tag.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_tag, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\"\n",
    "\n",
    "# loading tokenizers \n",
    "with open('tokenizer_char.pickle', 'rb') as handle:\n",
    "    tokenizer_char = pickle.load(handle)\n",
    "    \n",
    "with open('tokenizer_tag.pickle', 'rb') as handle:\n",
    "    tokenizer_tag = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, y_train) = process_data(train, tokenizer_char, tokenizer_tag, stem_max_len, tag_max_len, surface_token_max_len,\n",
    "#                                  sentence_max_len, num_max_analysis, exclude_unambigious, shuffle = True)\n",
    "#(X_dev, y_dev) = process_data(dev, tokenizer_char, tokenizer_tag, stem_max_len, tag_max_len, surface_token_max_len,\n",
    "#                                  sentence_max_len, num_max_analysis, exclude_unambigious, shuffle = True)\n",
    "(X_test, y_test) = process_data(test, tokenizer_char, tokenizer_tag, stem_max_len, tag_max_len, surface_token_max_len,\n",
    "                                  sentence_max_len, num_max_analysis, exclude_unambigious, shuffle = True)\n",
    "(X_test_2006, y_test_2006) = process_data(test_2006, tokenizer_char, tokenizer_tag, stem_max_len, tag_max_len, surface_token_max_len,\n",
    "                                  sentence_max_len, num_max_analysis, exclude_unambigious, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2,330,638\n"
     ]
    }
   ],
   "source": [
    "# Model Related\n",
    "char_vocab_size = len(tokenizer_char.word_index) + 1 # 94 + 1  = 95\n",
    "char_embed_size = 32\n",
    "stem_num_rnn_units = 128\n",
    "tag_vocab_size = len(tokenizer_tag.word_index) + 1 # 177 + 1 = 128\n",
    "tag_embed_size = 32\n",
    "tag_num_rnn_units = 128\n",
    "embed_join_type = 'add'\n",
    "dropout = 0.2\n",
    "num_rnn_stacks = 1\n",
    "\n",
    "model = create_model(num_max_analysis, stem_max_len, char_vocab_size, char_embed_size, stem_num_rnn_units,\n",
    "                     tag_max_len, tag_vocab_size, tag_embed_size, tag_num_rnn_units,\n",
    "                     sentence_max_len, surface_token_max_len, embed_join_type, dropout,\n",
    "                     num_rnn_stacks)\n",
    "print('Number of trainable parameters:', f'{model.count_params():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10148/10148 [==============================] - 2401s 236ms/step - loss: 0.4738 - accuracy: 0.8435 - val_loss: 0.2385 - val_accuracy: 0.9134\n",
      "\n",
      "Epoch 00001: saving model to saved_models\\weights.01-0.2385.hdf5\n",
      "Epoch 2/100\n",
      "10148/10148 [==============================] - 2402s 237ms/step - loss: 0.2679 - accuracy: 0.9070 - val_loss: 0.2065 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00002: saving model to saved_models\\weights.02-0.2065.hdf5\n",
      "Epoch 3/100\n",
      "10148/10148 [==============================] - 2399s 236ms/step - loss: 0.2330 - accuracy: 0.9196 - val_loss: 0.1872 - val_accuracy: 0.9349\n",
      "\n",
      "Epoch 00003: saving model to saved_models\\weights.03-0.1872.hdf5\n",
      "Epoch 4/100\n",
      "10148/10148 [==============================] - 2391s 236ms/step - loss: 0.2088 - accuracy: 0.9281 - val_loss: 0.1929 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00004: saving model to saved_models\\weights.04-0.1929.hdf5\n",
      "Epoch 5/100\n",
      "10148/10148 [==============================] - 2390s 235ms/step - loss: 0.1903 - accuracy: 0.9356 - val_loss: 0.1881 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00005: saving model to saved_models\\weights.05-0.1881.hdf5\n",
      "Epoch 6/100\n",
      "10148/10148 [==============================] - 2386s 235ms/step - loss: 0.1787 - accuracy: 0.9399 - val_loss: 0.1832 - val_accuracy: 0.9416\n",
      "\n",
      "Epoch 00006: saving model to saved_models\\weights.06-0.1832.hdf5\n",
      "Epoch 7/100\n",
      "10148/10148 [==============================] - 2410s 237ms/step - loss: 0.1667 - accuracy: 0.9439 - val_loss: 0.1835 - val_accuracy: 0.9424\n",
      "\n",
      "Epoch 00007: saving model to saved_models\\weights.07-0.1835.hdf5\n",
      "Epoch 8/100\n",
      "10148/10148 [==============================] - 2388s 235ms/step - loss: 0.1589 - accuracy: 0.9467 - val_loss: 0.1819 - val_accuracy: 0.9457\n",
      "\n",
      "Epoch 00008: saving model to saved_models\\weights.08-0.1819.hdf5\n",
      "Epoch 9/100\n",
      "10148/10148 [==============================] - 2383s 235ms/step - loss: 0.1537 - accuracy: 0.9486 - val_loss: 0.1936 - val_accuracy: 0.9439\n",
      "\n",
      "Epoch 00009: saving model to saved_models\\weights.09-0.1936.hdf5\n",
      "Epoch 10/100\n",
      "10148/10148 [==============================] - 2378s 234ms/step - loss: 0.1470 - accuracy: 0.9507 - val_loss: 0.1842 - val_accuracy: 0.9468\n",
      "\n",
      "Epoch 00010: saving model to saved_models\\weights.10-0.1842.hdf5\n",
      "Epoch 11/100\n",
      "10148/10148 [==============================] - 2379s 234ms/step - loss: 0.1407 - accuracy: 0.9530 - val_loss: 0.1879 - val_accuracy: 0.9468\n",
      "\n",
      "Epoch 00011: saving model to saved_models\\weights.11-0.1879.hdf5\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xf2c7c568c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "verbose = 1\n",
    "patience = 3\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = patience, verbose = verbose)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = CHECKPOINT_DIR, \n",
    "                                                save_freq = 'epoch', save_weights_only = True, verbose = 1)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
    "\n",
    "# Continuing from checkpoint\n",
    "files = os.listdir('saved_models')\n",
    "files = [file for file in files if \".hdf5\" in file]\n",
    "if len(files) > 0:\n",
    "    last_epoch_was = pd.Series(files).str.split('.').str[1].str.split('-').str[0].astype(int).max()\n",
    "    idx_of_weight_to_load = pd.Series(files).str.split('.').str[1].str.split('-').str[0].astype(int).idxmax()\n",
    "\n",
    "    weight_to_load = files[idx_of_weight_to_load]\n",
    "    print('Loading checkpoint of last epoch:', weight_to_load)\n",
    "    model.load_weights('saved_models/' + weight_to_load)\n",
    "else:\n",
    "    last_epoch_was = 0\n",
    "\n",
    "# Fitting\n",
    "model.fit(data_generator(train_files, batch_size, tokenizer_char, tokenizer_tag, stem_max_len, tag_max_len, surface_token_max_len, \n",
    "                sentence_max_len, num_max_analysis, exclude_unambigious, shuffle), epochs = epochs, batch_size = batch_size, verbose = verbose, \n",
    "          callbacks = [checkpoint, early_stopping], shuffle = False, validation_data = (X_test, y_test),\n",
    "          steps_per_epoch = (1_299_050 // batch_size), initial_epoch = last_epoch_was)\n",
    "# 837_518 is the number of valid tokens in OnurGungor data\n",
    "# 1_299_050 is for trmor2006, trmor2016, trmor2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: weights.08-0.1751.hdf5\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('saved_models')\n",
    "files = [file for file in files if \".hdf5\" in file]\n",
    "files_ = pd.Series(files).str.split(\".\", expand = True)\n",
    "idx_of_min_weight = files_.loc[:, 2].astype(int).idxmin()\n",
    "weight_to_load = files[idx_of_min_weight]\n",
    "print('Loading checkpoint:', weight_to_load)\n",
    "model.load_weights('saved_models/' + weight_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrMorph2006 test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguity_levels = []\n",
    "for idx in range(X_test_2006[0].shape[0]):\n",
    "    ambiguity_level = (X_test_2006[0][idx].sum(axis = 1) != 0).sum()\n",
    "    ambiguity_levels.append(ambiguity_level)\n",
    "ambiguity_levels = np.array(ambiguity_levels)\n",
    "\n",
    "ambigious_indices = ambiguity_levels != 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2x128 gru results: \n",
    "    - [0.20262235403060913, 0.9410681128501892]\n",
    "    - [0.12822052836418152, 0.9628770351409912]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 6s 30ms/step - loss: 0.1966 - accuracy: 0.9364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17724522948265076, 0.9429097771644592]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results of ambigious ones only.\n",
    "# This is 91.03 on Shen et al.\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.evaluate([X[ambigious_indices] for X in X_test_2006], y_test_2006[ambigious_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 30ms/step - loss: 0.1120 - accuracy: 0.9640 0s - loss: 0.115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11200806498527527, 0.9640371203422546]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results with all.\n",
    "# This is 96.41 on Shen et al.\n",
    "model.evaluate(X_test_2006, y_test_2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrMorph2006 and TrMorph2018 test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2x128 gru results: \n",
    "    - [0.2547953724861145, 0.924221932888031]\n",
    "    - [0.19014544785022736, 0.9436220526695251]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguity_levels = []\n",
    "for idx in range(X_test[0].shape[0]):\n",
    "    ambiguity_level = (X_test[0][idx].sum(axis = 1) != 0).sum()\n",
    "    ambiguity_levels.append(ambiguity_level)\n",
    "ambiguity_levels = np.array(ambiguity_levels)\n",
    "\n",
    "ambigious_indices = ambiguity_levels != 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 [==============================] - 18s 29ms/step - loss: 0.2495 - accuracy: 0.9258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24949999153614044, 0.9258257150650024]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results of ambigious ones only.\n",
    "model.evaluate([X[ambigious_indices] for X in X_test], y_test[ambigious_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839/839 [==============================] - 24s 28ms/step - loss: 0.1754 - accuracy: 0.9432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17537835240364075, 0.9432119131088257]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results with all.\n",
    "# This is 96.41 on Shen et al.\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
